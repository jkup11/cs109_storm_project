{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, first we import the necessary libraries.  In particular, we use [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) to give us a nicer default color palette, with our plots being of large (`poster`) size and with a white-grid background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29532\n",
       "1    29565\n",
       "2    29543\n",
       "3    29565\n",
       "4    29592\n",
       "Name: BEGIN_ZIP, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_data = pd.read_csv(\"AfterGotZips2016.csv\", \n",
    "                      encoding = \"ISO-8859-1\")\n",
    "zip_data[\"BEGIN_ZIP\"].head()\n",
    "\n",
    "# # Get rid of ZIP codes just for PO boxes\n",
    "# zip_data = zip_data.loc[zip_data['Type'] == 'Standard']\n",
    "# zip_data = zip_data.reset_index(drop=True)\n",
    "\n",
    "# zips = zip_data[\"ZIP Code\"]\n",
    "\n",
    "# for i in range(0, len(zips)):\n",
    "#     zips[i] = str(zips[i])[9:]\n",
    "# #print(len(zips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<requests.sessions.Session object at 0x120da5b00>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "s = requests.session()\n",
    "#s.headers['user-agent'] = 'Mozilla/5.0'\n",
    "print(s) \n",
    "r = s.get('https://factfinder.census.gov/rest/communityFactsNav/nav', params={'searchTerm': 99501})\n",
    "r\n",
    "type(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['isNotValidGeo', 'displayNoDataAvailableMsg', 'eusbreadcrumb', 'currentContext', 'breadcrumbTitle', 'currentContextURI', 'backToBreadcrumbTitle', 'measureAndLinksContent', 'leftNavSelection', 'disambiguationContent'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json().keys()\n",
    "r.json()['CFMetaData'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div>\\n<h2>99501</h2>\\n<div class=\\'datapoint\\'>\\n<div class=\"actionbar\">\\n<div class=\"buttons\">\\n<a id=\"bookmark_btn\" class=\"ab-bookmark\" href=\"javascript:openCFBookmark();\" onmouseover=\"ab_in(this)\" onmouseout=\"ab_out(this)\"><span></span> Bookmark/Save</a>\\n<img src=\"/common/img/button_sep.gif\" alt=\"\" width=\"3\" height=\"18\" />\\n<a id=\"print_btn\" class=\"ab-print\" href=\"javascript:printCF();\" onmouseover=\"ab_in(this)\" onmouseout=\"ab_out(this)\"><span></span> Print</a>\\n</div>\\n</div>\\n<h3>\\nPopulation  </h3>\\n<div style=\"margin:0 0 10px 0\">\\n<select onchange=\"selectMeasure(\\'POPULATION\\', this);\" style=\"font-size:93%\">\\n<option value=\"DECENNIAL_CNT\"  selected=\"selected\">Census 2010 Total Population</option><option value=\"PEP_EST\" >2016 Population Estimate (as of July 1, 2016)</option><option value=\"ACS_EST\" >2015 ACS 5-Year Population Estimate</option></select>\\n</div>\\n  <div class=\\'value\\'>17,603 </div>\\n<div class=\\'source\\'>Source: <a href=\"http://factfinder.census.gov/bkmk/table/1.0/en/DEC/10_DP/DPDP1/8600000US99501\">2010 Demographic Profile</a></div>\\n</div>\\n</div>\\n<div>\\n<h2>Popular tables for this geography:</h2>\\n<div class=\"links\">\\n<h3>\\n2010 Census\\n</h3>\\n<ul>\\n<li>\\n<div class=\"linkswithhyper\">\\n<a href=\"/bkmk/table/1.0/en/DEC/10_DP/DPDP1/8600000US99501\">General Population and Housing Characteristics (Population, Age, Sex, Race, Households and Housing, ...)</a>\\n</div>\\n</li>\\n<li>\\n<div class=\"linkswithhyper\">\\n<a href=\"/bkmk/table/1.0/en/DEC/10_SF1/QTP3/8600000US99501\">Race and Hispanic or Latino Origin</a>\\n</div>\\n</li>\\n<li>\\n<div class=\"linkswithhyper\">\\n<a href=\"/bkmk/table/1.0/en/DEC/10_SF1/QTP10/8600000US99501\">Hispanic or Latino by Type (Mexican, Puerto Rican, ...)</a>\\n</div>\\n</li>\\n<li>\\n<div class=\"linkswithhyper\">\\n<a href=\"/bkmk/table/1.0/en/DEC/10_SF1/QTP11/8600000US99501\">Households and Families (Relationships, Children, Household Size, ...)</a>\\n</div>\\n</li>\\n</ul>\\n<h3>\\n2015 American Community Survey\\n</h3>\\n<ul>\\n<li>\\n<div class=\"linkswithhyper\">\\n<a href=\"/bkmk/table/1.0/en/ACS/15_5YR/DP05/8600000US99501\">Demographic and Housing Estimates (Age, Sex, Race, Households and Housing, ...)</a>\\n</div>\\n</li>\\n</ul>\\n<h3>\\n2016 Population Estimates Program\\n</h3>\\n<ul>\\n<li>\\n<div class=\"linksnohyper\">\\n<a title=\"This table is not available for your selected geography.\">Annual Population Estimates</a>\\n</div>\\n</li>\\n</ul>\\n<h3>\\nCensus 2000\\n</h3>\\n<ul>\\n<li>\\n<div class=\"linkswithhyper\">\\n<a href=\"/bkmk/table/1.0/en/DEC/00_SF1/DP1/8600000US99501\">General Demographic Characteristics (Population, Age, Sex, Race, Households and Housing, ...)</a>\\n</div>\\n</li>\\n</ul>\\n</div>\\n<div class=\"morelinks\">\\n<img src=\"/common/img/bullet_sm_gold.png\" style=\"margin: 0 2px 1px 0;\" />\\n<a href=\"javascript:keepSelectionList(\\'datafinder\\',\\'t\\');\">Want more?</a>\\nNeed help? Use\\n<a href=\"javascript:processTransition(\\'navwiz\\');\">Guided Search</a>\\nor visit Census.gov\\'s\\n<a href=\"//www.census.gov/quickfacts/\" target=\"_blank\">Quick Facts</a>.\\n</div>\\n</div>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()['CFMetaData']['measureAndLinksContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = r.json()['CFMetaData']['measureAndLinksContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['displayID', 'displayLabel', 'productDataset', 'eusbreadcrumb', 'currentContext', 'breadcrumbTitle', 'currentContextURI', 'backToBreadcrumbTitle', 'universe', 'tableToolsAvailable', 'tableToolsEnabled', 'mappable', 'statsigSupported', 'statsigEnabled', 'disableCSVForNP', 'downloadOnly', 'showHideEnabled', 'previousProductId', 'previousProductType', 'currentProductIndex', 'currentProductType', 'nextProductId', 'nextProductType', 'currentProductHasActiveMap', 'mapActiveInSession', 'totalProductsInSelection', 'renderForMap', 'renderForChart', 'isDocument', 'isNP', 'productRecSpec', 'totalColumnCount', 'presentationDownloadColLimit', 'totalRowCount', 'chartRendered', 'presentationDownloadRowLimit', 'productDataTable', 'requestRejected', 'requestRejectionMessage', 'noDataFound', 'viewAllLimit', 'tableModified', 'resetGeoOverlay', 'headerNotes', 'footerNotes', 'layoutHeaderNotes', 'layoutFooterNotes'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.get('https://factfinder.census.gov/bkmk/table/1.0/en/DEC/10_DP/DPDP1/8600000US99501')\n",
    "r = s.get('https://factfinder.census.gov/tablerestful/tableServices/renderProductData')\n",
    "r.json().keys()\n",
    "#['ProductData']\n",
    "r.json()['ProductData'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<div class='actionbar'><div id='notes-div1' class=\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "s = requests.session()\n",
    "s.headers['user-agent'] = 'Mozilla/5.0'\n",
    "\n",
    "s.get('https://factfinder.census.gov/bkmk/table/1.0/en/DEC/10_DP/DPDP1/8600000US99501')\n",
    "#<Response [200]>\n",
    "r = s.get('https://factfinder.census.gov/tablerestful/tableServices/renderProductData')\n",
    "r.json().keys()\n",
    "#['ProductData']\n",
    "r.json()['ProductData']['productDataTable'][:50]\n",
    "#\"<div class='actionbar'><div id='notes-div1' class=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from   bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# This is just to get the column names\n",
    "\n",
    "zipcodes = ['32963']\n",
    "\n",
    "base   = 'https://factfinder.census.gov/'\n",
    "report = base + 'bkmk/table/1.0/en/DEC/10_DP/DPDP1/8600000US'\n",
    "render = base + 'tablerestful/tableServices/renderProductData'\n",
    "\n",
    "with requests.session() as s:\n",
    "\n",
    "    for zipcode in zipcodes:\n",
    "        s.get(report + zipcode)\n",
    "        r = s.get(render)\n",
    "\n",
    "        html = r.json()['ProductData']['productDataTable']\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        rows = soup.find(\"table\").find_all(\"tr\")[2:217]\n",
    "        \n",
    "        my_cols = []\n",
    "        \n",
    "        for row in rows:\n",
    "            headers = row.find_all(\"th\")\n",
    "            for header in headers:\n",
    "                actual_header = header.get_text()\n",
    "                count = actual_header + \"count\"\n",
    "                perc = actual_header + \"percent\"\n",
    "                my_cols.append(count)\n",
    "                my_cols.append(perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_cols_df = pd.DataFrame(my_cols)\n",
    "# my_cols_df\n",
    "\n",
    "# my_data_df = pd.DataFrame(my_data)\n",
    "# #my_data_df\n",
    "\n",
    "# my_cols_df[\"32963\"] = my_data\n",
    "# my_cols_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknown_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009\n",
      "25265\n"
     ]
    }
   ],
   "source": [
    "print(len(zip_data[\"BEGIN_ZIP\"][24256:]))\n",
    "print(len(zip_data[\"BEGIN_ZIP\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;31m# sending a valid response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[1;32m    267\u001b[0m                                      \" response\")\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m                 )\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    642\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 643\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;31m# sending a valid response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[1;32m    267\u001b[0m                                      \" response\")\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b4fd57097c0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mzipcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    486\u001b[0m         }\n\u001b[1;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))"
     ]
    }
   ],
   "source": [
    "# This part is just extracting data then\n",
    "\n",
    "\n",
    "zipcodes = zip_data[\"BEGIN_ZIP\"][24256:]\n",
    "\n",
    "scraped = pd.DataFrame()\n",
    "\n",
    "base   = 'https://factfinder.census.gov/'\n",
    "report = base + 'bkmk/table/1.0/en/DEC/10_DP/DPDP1/8600000US'\n",
    "render = base + 'tablerestful/tableServices/renderProductData'\n",
    "\n",
    "with requests.session() as s:\n",
    "    #s.headers['user-agent'] = 'Mozilla/5.0'\n",
    "\n",
    "    for zipcode in zipcodes:\n",
    "        #print(zipcode)\n",
    "        \n",
    "        if zipcode == \"Unknown\":\n",
    "            unknown_counter = unknown_counter + 1\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            time.sleep(3)\n",
    "            s.get(report + zipcode)\n",
    "            r = s.get(render)\n",
    "\n",
    "            html = r.json()['ProductData']['productDataTable']\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            #print(soup)\n",
    "\n",
    "            rows = soup.find(\"table\").find_all(\"tr\")[2:217]\n",
    "            #rows = soup.find(\"table\").find_all(\"tr\")[2]\n",
    "            #print(rows)\n",
    "            #headings = [tr.get_text() for th in soup.find(\"table\").find_all(\"tr\").find_all(\"tr\")]\n",
    "            #print(headings)\n",
    "\n",
    "            my_cols = []\n",
    "            my_data = []\n",
    "            data_zipcode = {}\n",
    "\n",
    "            for row in rows:\n",
    "                #print(\"inhere\")\n",
    "                #print(row)\n",
    "                headers = row.find_all(\"th\")\n",
    "                for header in headers:\n",
    "                    actual_header = header.get_text()\n",
    "\n",
    "                    count = actual_header + \"count\"\n",
    "                    perc = actual_header + \"percent\"\n",
    "\n",
    "                    my_cols.append(count)\n",
    "                    my_cols.append(perc)\n",
    "\n",
    "\n",
    "\n",
    "                    #print(header.get_text())\n",
    "                #print(header)\n",
    "\n",
    "                datums = row.find_all(\"td\") # + row.find_all(\"th\")\n",
    "                #print(datums)\n",
    "\n",
    "                actual_data = []\n",
    "\n",
    "                for datum in datums:\n",
    "                    #print(datum)\n",
    "                    actual_data.append(datum.get_text())\n",
    "                    my_data.append(datum.get_text())\n",
    "                    #print(datum.get_text())\n",
    "                #print(actual_data)\n",
    "\n",
    "                data_zipcode[actual_header] = actual_data\n",
    "            #print(my_data)\n",
    "            #print(zipcode)\n",
    "            my_data = [zipcode] + my_data\n",
    "            #print(my_data)\n",
    "            scraped[zipcode] = my_data\n",
    "    #         print(rows)\n",
    "    #         datums = rows.find_all(\"td\")[0]\n",
    "    #         print('now datums')\n",
    "    #         print(datums.get_text())\n",
    "\n",
    "            #display(HTML(soup.prettify()))\n",
    "        \n",
    "#print('the end')\n",
    "\n",
    "#data_zipcode\n",
    "#my_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scraped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if my_cols[0] != \"Zipcode\":\n",
    "    my_cols = [\"Zipcode\"] + my_cols\n",
    "better_scraped = scraped.transpose()\n",
    "better_scraped\n",
    "print(len(my_cols))\n",
    "print(better_scraped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "better_scraped.columns = my_cols\n",
    "better_scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "better_scraped.to_csv(\"32_457_MonPM2010.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "better_scraped_remove_r = pd.read_csv(\"ScrapedZipCodeInfo.csv\", encoding = \"ISO-8859-1\")\n",
    "better_scraped_remove_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "better_scraped_remove_r.to_csv(\"ScrapedZipCodeInfo_NoR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = \"308,745,538(r38234)\"\n",
    "example.split(\"(\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "better_scraped_remove_r[\"Total populationcount\"][58:68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = open(\"laborforce_annual_2015.txt\", \"r\") \n",
    "file.read() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Scraping Wikipedia for Billboard Top 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question you will scrape Wikipedia for the Billboard's top 100 singles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Wikipedia for Billboard singles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using  [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/), and suggest that you use Python's built in `requests` library to fetch the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Parsing the Billboard Wikipedia page for 1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the web page at http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970 using a HTTP GET request. From this web page we'll extract the top 100 singles and their rankings. Create a list of dictionaries, 100 of them to be precise, with entries like \n",
    "\n",
    "`{'url': '/wiki/Sugarloaf_(band)', 'ranking': 30, 'band_singer': 'Sugarloaf', 'title': 'Green-Eyed Lady'}`. \n",
    "\n",
    "If you look at that web page, you'll see a link for every song, from which you can get the `url` of the singer or band. We will use these links later to scrape information about the singer or band. From the listing we can also get the band or singer name `band_singer`, and `title` of the song.\n",
    "\n",
    "*HINT: look for a table with class `wikitable`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "You should get something similar to this (where songs is the aforementioned list):\n",
    "\n",
    "```\n",
    "songs[2:4]\n",
    "```\n",
    "\n",
    "```\n",
    "[{'band_singer': 'The Guess Who',\n",
    "  'ranking': 3,\n",
    "  'title': '\"American Woman\"',\n",
    "  'url': '/wiki/The_Guess_Who'},\n",
    " {'band_singer': 'B.J. Thomas',\n",
    "  'ranking': 4,\n",
    "  'title': '\"Raindrops Keep Fallin\\' on My Head\"',\n",
    "  'url': '/wiki/B.J._Thomas'}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# The \"requests\" library makes working with HTTP requests easier\n",
    "# than the built-in urllib libraries.\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here we access the webpage and download the content using requests\n",
    "t1970=requests.get(\"http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970\")\n",
    "t1970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll just reuse the request object that was previously created to create a BeautifulSoup element.\n",
    "soup = BeautifulSoup(t1970.text, \"html.parser\")\n",
    "\n",
    "# In this line we are looking for a single \"table\" element with a class of wikitable;\n",
    "# and then looking for all the \"tr\" elements on that table (notice the find vs find_all calls).\n",
    "rows = soup.find(\"table\", attrs={\"class\": \"wikitable\"}).find_all(\"tr\")[1:]\n",
    "\n",
    "# We then define a function whose job it is to act on\n",
    "# each column's element in each row in the table.\n",
    "def cleaner(r):\n",
    "    ranking = int(r[0].get_text())\n",
    "    title = r[1].get_text()\n",
    "    band_singer = r[2].get_text()\n",
    "    url = r[2].find(\"a\").get(\"href\")\n",
    "    return [ranking, title, band_singer, url]\n",
    "\n",
    "\n",
    "# Next we'll create a list of names that will be used as dictionary keys.\n",
    "fields = [\"ranking\", \"title\", \"band_singer\", \"url\"]\n",
    "\n",
    "# We now use the cleaner function to process each \"td\" element on a given row.\n",
    "# It gives us a bunch of band information\n",
    "# The zip function creates a list of pairs; which the dict function then uses\n",
    "# to create a dictionary, using the first element of the pair as the key and the second as\n",
    "# the value; and finally, the list comprehension iterates over each row element, and puts\n",
    "# the result of each iteration on a list, which is then bound to the songs variable.\n",
    "songs = [dict(zip(fields, cleaner(row.find_all(\"td\")))) for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "songs[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Generalize the previous: scrape Wikipedia from 1992 to 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visiting the urls similar to the ones for 1970, we can obtain the billboard top 100 for the years 1992 to 2014. (We choose these later years rather than 1970 as you might find music from this era more interesting.) Download these using Python's `requests` module and store the text from those requests in a dictionary called `yearstext`. This dictionary ought to have as its keys the years (as integers from 1992 to 2014), and as values corresponding to these keys the text of the page being fetched.\n",
    "\n",
    "You ought to sleep a second (look up `time.sleep` in Python) at the very least in-between fetching each web page: you do not want Wikipedia to think you are a marauding bot attempting to mount a denial-of-service attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HINT: you might find `range` and string-interpolation useful to construct the URLs *.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "years=range(1992, 2015)\n",
    "print(years)\n",
    "yearstext={}\n",
    "for y in years:\n",
    "    print(y)\n",
    "    yreq=requests.get(\"http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_%i\" % y)\n",
    "    yearstext[y]=yreq.text\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parse and Clean data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the code you wrote to get data from 1970 which produces a list of dictionaries, one corresponding to each single.  Now write a function `parse_year(the_year, yeartext_dict)` which takes the year, prints it out, gets the text for the year from the just created `yearstext` dictionary, and return a list of dictionaries for that year, with one dictionary for each single. Store this list in the variable `yearinfo`.\n",
    "\n",
    "The dictionaries **must** be of this form:\n",
    "\n",
    "```\n",
    "{'band_singer': ['Brandy', 'Monica'],\n",
    "  'ranking': 2,\n",
    "  'song': ['The Boy Is Mine'],\n",
    "  'songurl': ['/wiki/The_Boy_Is_Mine_(song)'],\n",
    "  'titletext': '\" The Boy Is Mine \"',\n",
    "  'url': ['/wiki/Brandy_Norwood', '/wiki/Monica_(entertainer)']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spec of this function is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "parse_year\n",
    "\n",
    "Inputs\n",
    "------\n",
    "the_year: the year you want the singles for\n",
    "yeartext_dict: a dictionary with keys as integer years and values the downloaded web pages \n",
    "    from wikipedia for that year.\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "\n",
    "a list of dictionaries, each of which corresponds to a single and has the\n",
    "following data:\n",
    "\n",
    "Eg:\n",
    "\n",
    "{'band_singer': ['Brandy', 'Monica'],\n",
    "  'ranking': 2,\n",
    "  'song': ['The Boy Is Mine'],\n",
    "  'songurl': ['/wiki/The_Boy_Is_Mine_(song)'],\n",
    "  'titletext': '\" The Boy Is Mine \"',\n",
    "  'url': ['/wiki/Brandy_Norwood', '/wiki/Monica_(entertainer)']}\n",
    "  \n",
    "A dictionary with the following data:\n",
    "    band_singer: a list of bands/singers who made this single\n",
    "    song: a list of the titles of songs on this single\n",
    "    songurl: a list of the same size as song which has urls for the songs on the single \n",
    "        (see point 3 above)\n",
    "    ranking: ranking of the single\n",
    "    titletext: the contents of the table cell\n",
    "    band_singer: a list of bands or singers on this single\n",
    "    url: a list of wikipedia singer/band urls on this single: only put in the part \n",
    "        of the url from /wiki onwards\n",
    "    \n",
    "\n",
    "Notes\n",
    "-----\n",
    "See description and example above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful notes\n",
    "\n",
    "Notice that some singles might have multiple songs:\n",
    "\n",
    "```\n",
    "{'band_singer': ['Jewel'],\n",
    "  'ranking': 2,\n",
    "  'song': ['Foolish Games', 'You Were Meant for Me'],\n",
    "  'songurl': ['/wiki/Foolish_Games',\n",
    "   '/wiki/You_Were_Meant_for_Me_(Jewel_song)'],\n",
    "  'titletext': '\" Foolish Games \" / \" You Were Meant for Me \"',\n",
    "  'url': ['/wiki/Jewel_(singer)']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some singles don't have a song URL:\n",
    "\n",
    "```\n",
    "{'band_singer': [u'Nu Flavor'],\n",
    "  'ranking': 91,\n",
    "  'song': [u'Heaven'],\n",
    "  'songurl': [None],\n",
    "  'titletext': u'\"Heaven\"',\n",
    "  'url': [u'/wiki/Nu_Flavor']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus there are some issues this function must handle:\n",
    "\n",
    "1. There can be more than one  `band_singer` as can be seen above (sometimes with a comma, sometimes with \"featuring\" in between). The best way to parse these is to look for the urls.\n",
    "2. There can be two songs in a single, because of the way the industry works: there are two-sided singles. See https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1997 for an example. You can find other examples in 1998 and 1999.\n",
    "3. The `titletext` is the contents of the table cell, and retains the quotes that Wikipedia puts on the single.\n",
    "4. If no song anchor is found (see the 24th song in the above url), assume there is one song in the single, set `songurl` to [`None`] and the song name to the contents of the table cell with the quotes stripped (ie `song` is a one-element list with this the `titletext` stripped of its quotes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, we can do this for 1997. We'll print the first 5 outputs: `parse_year(1997, yearstext)[:5]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give the following. Notice that the year 1997 exercises the edge cases we talked about earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[{'band_singer': ['Elton John'],\n",
    "  'ranking': 1,\n",
    "  'song': ['Something About the Way You Look Tonight',\n",
    "   'Candle in the Wind 1997'],\n",
    "  'songurl': ['/wiki/Something_About_the_Way_You_Look_Tonight',\n",
    "   '/wiki/Candle_in_the_Wind_1997'],\n",
    "  'titletext': '\" Something About the Way You Look Tonight \" / \" Candle in the Wind 1997 \"',\n",
    "  'url': ['/wiki/Elton_John']},\n",
    " {'band_singer': ['Jewel'],\n",
    "  'ranking': 2,\n",
    "  'song': ['Foolish Games', 'You Were Meant for Me'],\n",
    "  'songurl': ['/wiki/Foolish_Games',\n",
    "   '/wiki/You_Were_Meant_for_Me_(Jewel_song)'],\n",
    "  'titletext': '\" Foolish Games \" / \" You Were Meant for Me \"',\n",
    "  'url': ['/wiki/Jewel_(singer)']},\n",
    " {'band_singer': ['Puff Daddy', 'Faith Evans', '112'],\n",
    "  'ranking': 3,\n",
    "  'song': [\"I'll Be Missing You\"],\n",
    "  'songurl': ['/wiki/I%27ll_Be_Missing_You'],\n",
    "  'titletext': '\" I\\'ll Be Missing You \"',\n",
    "  'url': ['/wiki/Sean_Combs', '/wiki/Faith_Evans', '/wiki/112_(band)']},\n",
    " {'band_singer': ['Toni Braxton'],\n",
    "  'ranking': 4,\n",
    "  'song': ['Un-Break My Heart'],\n",
    "  'songurl': ['/wiki/Un-Break_My_Heart'],\n",
    "  'titletext': '\" Un-Break My Heart \"',\n",
    "  'url': ['/wiki/Toni_Braxton']},\n",
    " {'band_singer': ['Puff Daddy', 'Mase'],\n",
    "  'ranking': 5,\n",
    "  'song': [\"Can't Nobody Hold Me Down\"],\n",
    "  'songurl': ['/wiki/Can%27t_Nobody_Hold_Me_Down'],\n",
    "  'titletext': '\" Can\\'t Nobody Hold Me Down \"',\n",
    "  'url': ['/wiki/Sean_Combs', '/wiki/Mase']}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "fields = [\"ranking\", \"song\", \"songurl\", \"titletext\", \"band_singer\", \"url\"]\n",
    "\n",
    "# Helper functions.\n",
    "def get_cols(row):\n",
    "    return row.find_all(\"th\") + row.find_all(\"td\")\n",
    "\n",
    "def break_a(col):\n",
    "    return list(map(list, zip(*[(a.get(\"title\").strip('\"'), a.get(\"href\")) for a in col.find_all(\"a\")]))) \\\n",
    "            or [[col.get_text().strip('\"')], [None]]\n",
    "    \n",
    "def parse_cols(cols):\n",
    "    return [int(cols[0].get_text())] + break_a(cols[1]) + [cols[1].get_text()] + break_a(cols[2])\n",
    "\n",
    "def create_dict(cols):\n",
    "    return dict(zip(fields, cols))\n",
    "\n",
    "# Parser function.\n",
    "def parse_year(year, yearstext):\n",
    "    soup = BeautifulSoup(yearstext[year], 'html.parser')\n",
    "    rows = soup.find(\"table\", attrs={\"class\": \"wikitable\"}).find_all(\"tr\")[1:]\n",
    "    return [create_dict(parse_cols(get_cols(row))) for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parse_year(1997, yearstext)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yearinfo = {y:parse_year(y, yearstext) for y in years}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a json file of information from the scraped files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to lose all this work, so let's save the last data structure we created to disk. That way if you need to re-run from here, you don't need to redo all these requests and parsing. \n",
    "\n",
    "DO NOT RERUN THE HTTP REQUESTS TO WIKIPEDIA WHEN SUBMITTING.\n",
    "\n",
    "*We **DO NOT** need to see these JSON files in your submission!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "fd = open(\"/tmp/yearinfo.json\",\"w\")\n",
    "json.dump(yearinfo, fd)\n",
    "fd.close()\n",
    "del yearinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reload our JSON file into the yearinfo variable, just to be sure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RERUN WHEN SUBMITTING\n",
    "# Another way to deal with files. Has the advantage of closing the file for you.\n",
    "with open(\"/tmp/yearinfo.json\", \"r\") as fd:\n",
    "    yearinfo = json.load(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Construct a year-song-singer dataframe from the yearly information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a dataframe `flatframe` from the `yearinfo`. The frame should be similar to the frame below.  Each row of the frame represents a song, and carries with it the chief properties of year, song, singer, and ranking.\n",
    "\n",
    "![](https://raw.githubusercontent.com/cs109/a-2017/master/hwassets/images/HW1SC1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the dataframe, we'll need to iterate over the years and the singles per year. Notice how, above, the dataframe is ordered by ranking and then year. While the exact order is up to you, note that you will have to come up with a scheme to order the information.\n",
    "\n",
    "Check that the dataframe has sensible data types. You will also likely find that the year field has become an \"object\" (Pandas treats strings as generic objects): this is due to the conversion to and back from JSON. Such conversions need special care.\n",
    "\n",
    "(As an aside, we used the name `flatframe` to indicate that this dataframe is flattened from a hierarchical dictionary structure with the keys being the years.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix any data type issues with `flatframe`. (See Pandas [astype](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html) function.)\n",
    "\n",
    "We will use this `flatframe` in the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIPLE WAYS TO DO THIS. HERE IS ONE. ANOTHER WAY IS TO PUT YEAR IN EVERY DICTIONARY IN THE LIST, GET RID OF TOP LEVEL DICT, AND THEN EXPAND OUT THE SINGERS AND SONGS (CARTESIAN PRODUCT) AS BELOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yeardict={}\n",
    "for y in yearinfo.keys():\n",
    "    yearlist=yearinfo[y] # get list of singles for year\n",
    "    yearlist2=[]\n",
    "    for singledict in yearlist:\n",
    "        singers=singledict['band_singer']\n",
    "        for i, singer in enumerate(singers):\n",
    "            songs=singledict['song'] # generally one per single but maybe more\n",
    "            for j, song in enumerate(songs): # now inside each singer song combination\n",
    "                nd={}\n",
    "                nd['band_singer'] = singer\n",
    "                nd['url'] = singledict['url'][i]\n",
    "                nd['song'] = song\n",
    "                nd['songurl'] = singledict['songurl'][j]\n",
    "                nd['ranking'] = singledict['ranking']\n",
    "                yearlist2.append(nd)\n",
    "    yeardict[y]=pd.DataFrame(yearlist2) # one for each year\n",
    "yearspanel=pd.Panel.from_dict(yeardict, orient=\"minor\") # stack dataframes into a panel\n",
    "hierframe=yearspanel.to_frame() # flattening leads to a hierarchical index\n",
    "hierframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatframe = hierframe.reset_index()\n",
    "flatframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatframe = flatframe.rename(columns={'minor':'year'})\n",
    "del flatframe['major']\n",
    "flatframe.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatframe.year = flatframe.year.astype(int)\n",
    "flatframe.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who are the highest quality singers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the highest quality singers and plot them on a bar chart.\n",
    "\n",
    "#### 1.5 Find highest quality singers according to how prolific they are\n",
    "\n",
    "What do we mean by highest quality? This is of course open to interpretation, but let's define \"highest quality\" here as the number of times a singer appears in the top 100 over this time period. If a singer appears twice in a year (for different songs), this is counted as two appearances, not one. \n",
    "\n",
    "Make a bar-plot of the most prolific singers. Singers on this chart should have appeared at-least more than 15 times. (HINT: look at the docs for the pandas method `value_counts`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prolific=flatframe.url.value_counts()\n",
    "prolific[prolific > 15].plot(kind=\"barh\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 What if we used a different metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we would like to capture is this: a singer should to be scored higher if the singer appears higher in the rankings. So we'd say that a singer who appeared once at a higher and once at a lower ranking is a \"higher quality\" singer than one who appeared twice at a lower ranking. \n",
    "\n",
    "To do this, group all of a singers songs together and assign each song a score `101 - ranking`. Order the singers by their total score and make a bar chart for the top 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = flatframe.groupby('url').apply(lambda v: np.sum(101 - v['ranking']))\n",
    "scores= scores.sort_values(ascending=False)\n",
    "scores[:20].plot(kind=\"barh\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Do you notice any major differences when you change the metric?\n",
    "\n",
    "How have the singers at the top shifted places? Why do you think this happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n",
    "It seems Lil Wayne appeared in the top 100 many many times (31 times) but got beat out in terms of rankings by both Usher and Mariah Carey. Now we're in a position to answer the question that was posed in the beginning of this assignment -- who is a *better* singer, Mariah Carey or Rihanna? Well, it turns out Rihanna is more *prolific* by our definition, by quite a lot. So based on our analysis we have to give the title to Rihanna. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Scraping and Constructing: Information about Artists, Bands and Genres from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next job is to use those band/singer urls we collected under `flatframe.url` and get information about singers and/or bands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape information about artists from wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to fetch information about the singers or groups for all the winning songs in a list of years.\n",
    "\n",
    "Here we show a function that fetches information about a singer or group from their url on wikipedia. We create a cache object `urlcache` that will avoid redundant HTTP requests (e.g. an artist might have multiple singles on a single year, or be on the list over a span of years). Once we have fetched information about an artist, we don't need to do it again. The caching also helps if the network goes down, or the target website is having some problems. You simply need to run the `get_page` function below again, and the `urlcache` dictionary will continue to be filled.\n",
    "\n",
    "If the request gets an HTTP return code different from 200, (such as a 404 not found or 500 Internal Server Error) the cells for that URL will have a value of 1; and if the request completely fails (e.g. no network connection) the cell will have a value of 2. This will allow you to analyse the failed requests.\n",
    "\n",
    "Notice that we have wrapped the call in whats called _an exception block_. We try to make the request. If it fails entirely, or returns a HTTP code thats not 200, we set the status to 2 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urlcache={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    # Check if URL has already been visited.\n",
    "    if (url not in urlcache) or (urlcache[url]==1) or (urlcache[url]==2):\n",
    "        time.sleep(1)\n",
    "        # try/except blocks are used whenever the code could generate an exception (e.g. division by zero).\n",
    "        # In this case we don't know if the page really exists, or even if it does, if we'll be able to reach it.\n",
    "        try:\n",
    "            r = requests.get(\"http://en.wikipedia.org%s\" % url)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                urlcache[url] = r.text\n",
    "            else:\n",
    "                urlcache[url] = 1\n",
    "        except:\n",
    "            urlcache[url] = 2\n",
    "    return urlcache[url]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the `flatframe` by year, ascending, first. Think why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flatframe=flatframe.sort_values('year')\n",
    "flatframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulling and saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "# Here we are populating the url cache\n",
    "# subsequent calls to this cell should be very fast, since Python won't\n",
    "# need to fetch the page from the web server.\n",
    "# NOTE this function will take quite some time to run (about 30 mins for me), since we sleep 1 second before\n",
    "# making a request. If you run it again it will be almost instantaneous, save requests that might have failed\n",
    "# (you will need to run it again if requests fail..see cell below for how to test this)\n",
    "flatframe[\"url\"].apply(get_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to run this function again and again, in case there were network problems. Note that, because there is a \"global\" cache, it will take less time each time you run it. Also note that this function is designed to be run again and again: it attempts to make sure that there are no unresolved pages remaining. Let us make sure of this: *the sum below should be 0, and the boolean True.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "print(\"Number of bad requests:\",np.sum([(urlcache[k]==1) or (urlcache[k]==2) for k in urlcache])) # no one or 0's)\n",
    "print(\"Did we get all urls?\", len(flatframe.url.unique())==len(urlcache)) # we got all of the urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the `urlcache` to disk, just in case we need it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "with open(\"/tmp/artistinfo.json\",\"w\") as fd:\n",
    "    json.dump(urlcache, fd)\n",
    "del urlcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RERUN WHEN SUBMITTING\n",
    "with open(\"/tmp/artistinfo.json\") as json_file:\n",
    "    urlcache = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Extract information about singers and bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each page we collected about a singer or a band, extract the following information:\n",
    "\n",
    "1. If the page has the text \"Born\" in the sidebar on the right, extract the element with the class `.bday`. If the page doesn't contain \"Born\", store `False`.  Store either of these into the variable `born`. We want to analyze the artist's age.\n",
    "\n",
    "2. If the text \"Years active\" is found, but no \"born\", assume a band. Store into the variable `ya` the value of the next table cell corresponding to this, or `False` if the text is not found.\n",
    "\n",
    "Put this all into a function `singer_band_info` which takes the singer/band url as argument and returns a dictionary `dict(url=url, born=born, ya=ya)`.\n",
    "\n",
    "The information can be found on the sidebar on each such wikipedia page, as the example here shows:\n",
    "\n",
    "![sandg](https://raw.githubusercontent.com/cs109/a-2017/master/hwassets/images/sandg.png).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the function `singer_band_info` according to the following specification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the age at which singers achieve their top ranking. What conclusions can you draw from this distribution of ages?\n",
    "\n",
    "*HINT: You will need to do some manipulation of the `born` column, and find the song for which a band or an artist achieves their top ranking. You will then need to put these rows together into another dataframe or array to make the plot.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfb=largedf[largedf.born!=False][['year','born','ranking','url']]\n",
    "dfb['byear']=dfb.born.apply(lambda x: int(x.split('-')[0]))\n",
    "byurl=dfb.groupby('url')\n",
    "frames=[]\n",
    "for k, v in byurl:\n",
    "    minr=v.ranking.min()\n",
    "    frames.append(v[v.ranking==minr])\n",
    "topscoresdf=pd.concat(frames)\n",
    "(topscoresdf.year-topscoresdf.byear).hist(bins=np.arange(10,70,2), normed=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n",
    "The *age of first fame* has an interesting shape, unimodal with a mode in the late 20s with a long tail to the right. This makes sense because there really is a lower age limit to artists but not so much an upper one. So you expect this type of behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 At what year since inception do bands reach their top rankings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a similar calculation to plot a histogram of the years since inception at which bands reach their top ranking. What conclusions can you draw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "dfg=largedf[largedf.born==False]\n",
    "dfg=dfg[dfg.ya!=False]#drop em!\n",
    "dfg['start']=dfg.ya.apply(lambda x: int(x[:4]))\n",
    "frames2=[]\n",
    "for k, v in dfg.groupby('url'):\n",
    "    minr=v.ranking.min()\n",
    "    frames2.append(v[v.ranking==minr])\n",
    "tsdfg=pd.concat(frames2)\n",
    "(tsdfg.year-tsdfg.start).hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n",
    "There are many bands (a majority) that hit the top 100 within 8 years of inception. But there are still a large fraction of bands that take over 10 years to hit the top 100. I'm not saying that hitting this is the be-all and end-all, but it's at least an indication. But it's also important to keep the context in mind, many bands aren't together very long so if they were to hit the top-100 they would have to do it within the time that they are together. But all of this information is available on the Wikipedia pages, so we encourage further exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

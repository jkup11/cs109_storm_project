{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 8\n",
    "# Ensemble methods\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrollment: CS 109A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson Discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between a collision that produces Higgs bosons and collisions thats produce only background noise. We shall explore the use of ensemble methods for this classification task.\n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle colision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background). \n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: <a href = \"https://www.nature.com/articles/ncomms5308\">Baldi et al., Nature Communications 5, 2014</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('Higgs_train.csv')\n",
    "data_test = pd.read_csv('Higgs_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 29)\n",
      "(5000, 29)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_nda = data_train.as_matrix()\n",
    "\n",
    "X_train = data_train_nda[:, :-1]\n",
    "Y_train = data_train_nda[:, -1]\n",
    "\n",
    "data_test_nda = data_test.as_matrix()\n",
    "\n",
    "X_test = data_test_nda[:, :-1]\n",
    "Y_test = data_test_nda[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (2pt): Single Decision Tree\n",
    "We start by building a basic model which we will use as our base model for comparison. \n",
    "\n",
    "1. Fit a decision tree model to the training set and report the classification accuracy of the model on the test set. Use 5-fold cross-validation to choose the (maximum) depth for the tree. You will use the max_depth you find here throughout the homework. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Depths</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.623207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.620411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.635806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.639612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.626406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.629408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.627602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.625805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.615404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Depths    Scores\n",
       "0       2  0.623207\n",
       "1       3  0.620411\n",
       "2       4  0.635806\n",
       "3       5  0.639612\n",
       "4       6  0.626406\n",
       "5       7  0.629408\n",
       "6       8  0.627602\n",
       "7       9  0.625805\n",
       "8      10  0.615404"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# create an empty array to hold accuracy scores later\n",
    "scores = []\n",
    "depths = [2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# for each depth, fit the tree classifier onto the train set\n",
    "# use the cross_val_score method to directly perform cv and get the accuracy scores\n",
    "for depth in depths:\n",
    "    dt = DecisionTreeClassifier(max_depth = depth)\n",
    "    cv_scores = cross_val_score(dt, X_train, Y_train, cv=5)\n",
    "    scores.append(np.mean(cv_scores))\n",
    "\n",
    "# format into data series\n",
    "depths_se = pd.Series(depths)\n",
    "scores_se = pd.Series(scores)\n",
    "\n",
    "# create a dataframe for data presentation\n",
    "col_names = ['Depths', 'Scores']\n",
    "depths_scores_df = pd.DataFrame(columns = col_names)\n",
    "\n",
    "depths_scores_df['Depths'] = depths_se.values\n",
    "depths_scores_df['Scores'] = scores_se.values\n",
    "\n",
    "depths_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64559999999999995"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_depth = 5\n",
    "dt = DecisionTreeClassifier(max_depth = best_depth)\n",
    "dt.fit(X_train, Y_train)\n",
    "dt.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (15pt): Dropout-based Approach\n",
    "We start with a simple method inspired from the idea of 'dropout' in machine learning, where we fit multiple decision trees on random subsets of predictors, and combine them through a majority vote. The procedure is described below.\n",
    "\n",
    "- For each predictor in the training sample, set the predictor values to 0 with probability $p$  (i.e. drop the predictor by setting it to 0). Repeat this for $B$ trials to create $B$ separate training sets.\n",
    "\n",
    "\n",
    "- Fit decision tree models $\\hat{h}^1(x), \\ldots, \\hat{h}^B(x) \\in \\{0,1\\}$ to the $B$ training sets. \n",
    "\n",
    "- Combine the decision tree models into a single classifier by taking a majority vote:\n",
    "$$\n",
    "\\hat{H}_{maj}(x) \\,=\\, majority\\Big(\\hat{h}^1(x), \\ldots, \\hat{h}^B(x)\\Big).\n",
    "$$\n",
    "\n",
    "\n",
    "We shall refer to the combined classifier as an ** *ensemble classifier* **. Implement the described dropout approach, and answer the following questions:\n",
    "1. Apply the dropout procedure with $p = 0.5$ for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracy of the combined classifier. Does an increase in the number of trees improve the training and test performance? Explain your observations in terms of the bias-variance trade-off for the classifier.\n",
    "- Fix the number of trees to 64 and apply the dropout procedure with different dropout rates $p = 0.1, 0.3, 0.5, 0.7, 0.9$. Based on your results, explain how the dropout rate influences the bias and variance of the combined classifier.\n",
    "- Apply 5-fold cross-validation to choose the optimal combination of the dropout rate and number of trees. How does the test performance of an ensemble of trees fitted with the optimal dropout rate and number of trees compare with the single decision tree model in Question 1?\n",
    "[hint: Training with large number of trees can take long time. You may need to restrict the max number of trees.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# re-handle the data for safety, split to train and test\n",
    "\n",
    "data_train = pd.read_csv('Higgs_train.csv')\n",
    "data_test = pd.read_csv('Higgs_test.csv')\n",
    "\n",
    "data_train_nda = data_train.as_matrix()\n",
    "\n",
    "X_train = data_train_nda[:, :-1]\n",
    "Y_train = data_train_nda[:, -1]\n",
    "\n",
    "data_test_nda = data_test.as_matrix()\n",
    "\n",
    "X_test = data_test_nda[:, :-1]\n",
    "Y_test = data_test_nda[:, -1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "# best depth found in q1\n",
    "depth = 5\n",
    "\n",
    "# function to ensemble predict on our data \n",
    "def dropout_procedure(num_trees, dropout_rate, X, Y, X_test, Y_test):\n",
    "        \n",
    "    # num_trees is b- the number of models we build\n",
    "    train_predictions = np.zeros((num_trees, len(X)))\n",
    "    test_predictions = np.zeros((num_trees, len(X_test)))\n",
    "    final_train_prediction = []\n",
    "    final_test_prediction = []\n",
    "    \n",
    "    # loop through B times\n",
    "    for i in range(num_trees):        \n",
    "        model_X = copy.deepcopy(X)\n",
    "        model_X_test = copy.deepcopy(X_test)\n",
    "        for j in range(len(model_X[0])):\n",
    "            rand = random.uniform(0,1)\n",
    "            \n",
    "            # drop with the specified probability\n",
    "            if rand < dropout_rate:\n",
    "                # dropout column- set to 0 \n",
    "                for k in range(len(model_X)):\n",
    "                    model_X[k][j] = 0\n",
    "                \n",
    "        # fit classifier\n",
    "        dt = DecisionTreeClassifier(max_depth = depth)\n",
    "        dt.fit(model_X, Y)\n",
    "        \n",
    "        train_predict = dt.predict(model_X)\n",
    "        test_predict = dt.predict(model_X_test)\n",
    "        \n",
    "        # put tog\n",
    "        train_predictions[i,:] = train_predict\n",
    "        test_predictions[i,:] = test_predict\n",
    "        \n",
    "    \n",
    "    for i in range(len(model_X)):   \n",
    "        # add up ones\n",
    "        sum_one = 0\n",
    "        for j in range(num_trees):\n",
    "            sum_one = sum_one + train_predictions[j][i]\n",
    "        \n",
    "        # if more than .5 classified as 1, classify as 1, otherwise 0\n",
    "        if (1.0*sum_one/num_trees) > 0.5:\n",
    "            final_train_prediction.append(1.0)\n",
    "        else:\n",
    "            final_train_prediction.append(0.0)\n",
    "            \n",
    "    for i in range(len(model_X_test)):   \n",
    "        # add up ones\n",
    "        sum_one = 0\n",
    "        for j in range(num_trees):\n",
    "            sum_one = sum_one + test_predictions[j][i]\n",
    "        \n",
    "        # if more than .5 classified as 1, classify as 1, otherwise 0\n",
    "        if (1.0*sum_one/num_trees) > 0.5:\n",
    "            final_test_prediction.append(1.0)\n",
    "        else:\n",
    "            final_test_prediction.append(0.0)\n",
    "            \n",
    "    train_acc = accuracy_score(Y, final_train_prediction)\n",
    "    test_acc = accuracy_score(Y_test, final_test_prediction)\n",
    "        \n",
    "    return (train_acc, test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 2 trees and 0.5 dropout rate, train accuracy is 0.6924 and test accuracy is 0.653\n",
      "For 4 trees and 0.5 dropout rate, train accuracy is 0.6826 and test accuracy is 0.6538\n",
      "For 8 trees and 0.5 dropout rate, train accuracy is 0.7178 and test accuracy is 0.675\n",
      "For 16 trees and 0.5 dropout rate, train accuracy is 0.7112 and test accuracy is 0.6718\n",
      "For 32 trees and 0.5 dropout rate, train accuracy is 0.7292 and test accuracy is 0.6754\n",
      "For 64 trees and 0.5 dropout rate, train accuracy is 0.7356 and test accuracy is 0.6874\n",
      "For 128 trees and 0.5 dropout rate, train accuracy is 0.7338 and test accuracy is 0.6828\n",
      "For 256 trees and 0.5 dropout rate, train accuracy is 0.7398 and test accuracy is 0.6866\n"
     ]
    }
   ],
   "source": [
    "# part 1, for many different values of B\n",
    "for num_trees in [2,4,8,16,32,64,128,256]:\n",
    "    \n",
    "    train_acc, test_acc = dropout_procedure(num_trees, 0.5, X_train, Y_train, X_test, Y_test)\n",
    "    \n",
    "    print(\"For {} trees and 0.5 dropout rate, train accuracy is {} and test accuracy is {}\".format(num_trees, train_acc, test_acc))    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 64 trees and 0.1 dropout rate, train accuracy is 0.682 and test accuracy is 0.6456\n",
      "For 64 trees and 0.3 dropout rate, train accuracy is 0.7094 and test accuracy is 0.6674\n",
      "For 64 trees and 0.5 dropout rate, train accuracy is 0.7326 and test accuracy is 0.6838\n",
      "For 64 trees and 0.7 dropout rate, train accuracy is 0.7286 and test accuracy is 0.6788\n",
      "For 64 trees and 0.9 dropout rate, train accuracy is 0.6474 and test accuracy is 0.5976\n"
     ]
    }
   ],
   "source": [
    "# part 2, for many different values of dropout_rate\n",
    "num_trees = 64\n",
    "for dropout_rate in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    \n",
    "    train_acc, test_acc = dropout_procedure(num_trees, dropout_rate, X_train, Y_train, X_test, Y_test)\n",
    "    \n",
    "    print(\"For 64 trees and {} dropout rate, train accuracy is {} and test accuracy is {}\".format(dropout_rate, train_acc, test_acc))   \n",
    "       \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 2 trees and 0.1 dropout rate, average validation accuracy is 0.6441200000000001\n",
      "For 2 trees and 0.3 dropout rate, average validation accuracy is 0.632\n",
      "For 2 trees and 0.5 dropout rate, average validation accuracy is 0.61808\n",
      "For 2 trees and 0.7 dropout rate, average validation accuracy is 0.57516\n",
      "For 2 trees and 0.9 dropout rate, average validation accuracy is 0.54784\n",
      "For 4 trees and 0.1 dropout rate, average validation accuracy is 0.6548\n",
      "For 4 trees and 0.3 dropout rate, average validation accuracy is 0.66008\n",
      "For 4 trees and 0.5 dropout rate, average validation accuracy is 0.6439199999999999\n",
      "For 4 trees and 0.7 dropout rate, average validation accuracy is 0.6230800000000001\n",
      "For 4 trees and 0.9 dropout rate, average validation accuracy is 0.5504800000000001\n",
      "For 8 trees and 0.1 dropout rate, average validation accuracy is 0.64756\n",
      "For 8 trees and 0.3 dropout rate, average validation accuracy is 0.65892\n",
      "For 8 trees and 0.5 dropout rate, average validation accuracy is 0.66196\n",
      "For 8 trees and 0.7 dropout rate, average validation accuracy is 0.63992\n",
      "For 8 trees and 0.9 dropout rate, average validation accuracy is 0.5965199999999999\n",
      "For 16 trees and 0.1 dropout rate, average validation accuracy is 0.6485200000000001\n",
      "For 16 trees and 0.3 dropout rate, average validation accuracy is 0.6605999999999999\n",
      "For 16 trees and 0.5 dropout rate, average validation accuracy is 0.6696799999999999\n",
      "For 16 trees and 0.7 dropout rate, average validation accuracy is 0.65904\n",
      "For 16 trees and 0.9 dropout rate, average validation accuracy is 0.56368\n",
      "For 24 trees and 0.1 dropout rate, average validation accuracy is 0.64704\n",
      "For 24 trees and 0.3 dropout rate, average validation accuracy is 0.66636\n",
      "For 24 trees and 0.5 dropout rate, average validation accuracy is 0.67764\n",
      "For 24 trees and 0.7 dropout rate, average validation accuracy is 0.6632\n",
      "For 24 trees and 0.9 dropout rate, average validation accuracy is 0.5607599999999999\n",
      "For 32 trees and 0.1 dropout rate, average validation accuracy is 0.64764\n",
      "For 32 trees and 0.3 dropout rate, average validation accuracy is 0.6660000000000001\n",
      "For 32 trees and 0.5 dropout rate, average validation accuracy is 0.6768400000000001\n",
      "For 32 trees and 0.7 dropout rate, average validation accuracy is 0.66652\n",
      "For 32 trees and 0.9 dropout rate, average validation accuracy is 0.5617599999999999\n",
      "For 48 trees and 0.1 dropout rate, average validation accuracy is 0.6470400000000001\n",
      "For 48 trees and 0.3 dropout rate, average validation accuracy is 0.665\n",
      "For 48 trees and 0.5 dropout rate, average validation accuracy is 0.67556\n",
      "For 48 trees and 0.7 dropout rate, average validation accuracy is 0.66452\n",
      "For 48 trees and 0.9 dropout rate, average validation accuracy is 0.5652000000000001\n",
      "For 64 trees and 0.1 dropout rate, average validation accuracy is 0.64716\n",
      "For 64 trees and 0.3 dropout rate, average validation accuracy is 0.66684\n",
      "For 64 trees and 0.5 dropout rate, average validation accuracy is 0.67608\n",
      "For 64 trees and 0.7 dropout rate, average validation accuracy is 0.66384\n",
      "For 64 trees and 0.9 dropout rate, average validation accuracy is 0.56416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# 5 fold CV to select best combo of trees and dropout rate\n",
    "# in part 2.1, we hit our top accuacy at 64 trees, so we should cap it at 64\n",
    "\n",
    "data_train = pd.read_csv('Higgs_train.csv')\n",
    "data_test = pd.read_csv('Higgs_test.csv')\n",
    "\n",
    "tree_options = [2,4,8,16,24,32,48,64]\n",
    "dropout_rates = [0.1,0.3,0.5,0.7,0.9]\n",
    "\n",
    "for num_trees in tree_options:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        \n",
    "        n = len(X_train)\n",
    "        test_acc_total = 0\n",
    "        for f_train, f_test in KFold(n, n_folds=5, shuffle=False, random_state=11):\n",
    "            \n",
    "            train_data = data_train.iloc[f_train]\n",
    "            valid_data = data_train.iloc[f_test]\n",
    "            \n",
    "            data_train_nda = train_data.as_matrix()\n",
    "            data_valid_nda = valid_data.as_matrix()\n",
    "\n",
    "            train_x = data_train_nda[:, :-1]\n",
    "            train_y = data_train_nda[:, -1]\n",
    "            valid_x = data_test_nda[:, :-1]\n",
    "            valid_y = data_test_nda[:, -1]\n",
    "\n",
    "            train_acc, valid_acc = dropout_procedure(num_trees, dropout_rate, train_x, train_y, valid_x, valid_y)  \n",
    "            test_acc_total += valid_acc\n",
    "        test_acc_final = test_acc_total / 5.0\n",
    "        \n",
    "        print(\"For {} trees and {} dropout rate, average validation accuracy is {}\".format(num_trees, dropout_rate, test_acc_final))   \n",
    "            \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For optimal trees: 48 and optimal dropout rate: 0.5, train accuracy is 0.7348 and test accuracy is 0.682\n"
     ]
    }
   ],
   "source": [
    "# in the previous problem, the highest validation accuracy is for 48 trees and 0.5 dropout rate\n",
    "\n",
    "# re-handle the data for safety, split to train and test\n",
    "\n",
    "data_train = pd.read_csv('Higgs_train.csv')\n",
    "data_test = pd.read_csv('Higgs_test.csv')\n",
    "\n",
    "data_train_nda = data_train.as_matrix()\n",
    "\n",
    "X_train = data_train_nda[:, :-1]\n",
    "Y_train = data_train_nda[:, -1]\n",
    "\n",
    "data_test_nda = data_test.as_matrix()\n",
    "\n",
    "X_test = data_test_nda[:, :-1]\n",
    "Y_test = data_test_nda[:, -1]\n",
    "\n",
    "optimal_trees = 48\n",
    "optimal_dropout_rate = 0.5\n",
    "\n",
    "train_acc, test_acc = dropout_procedure(optimal_trees, optimal_dropout_rate, X_train, Y_train, X_test, Y_test)\n",
    "print(\"For optimal trees: {} and optimal dropout rate: {}, train accuracy is {} and test accuracy is {}\".format(optimal_trees, optimal_dropout_rate, train_acc, test_acc))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: \n",
    "\n",
    "#### Does an increase in the number of trees improve the training and test performance? Explain your observations in terms of the bias-variance trade-off for the classifier.\n",
    "\n",
    "Results: \n",
    "\n",
    "For 2 trees and 0.5 dropout rate, train accuracy is 0.6592 and test accuracy is 0.6256\n",
    "\n",
    "For 4 trees and 0.5 dropout rate, train accuracy is 0.6952 and test accuracy is 0.6424\n",
    "\n",
    "For 8 trees and 0.5 dropout rate, train accuracy is 0.7128 and test accuracy is 0.663\n",
    "\n",
    "For 16 trees and 0.5 dropout rate, train accuracy is 0.7326 and test accuracy is 0.6846\n",
    "\n",
    "For 32 trees and 0.5 dropout rate, train accuracy is 0.7354 and test accuracy is 0.6782\n",
    "\n",
    "For 64 trees and 0.5 dropout rate, train accuracy is 0.7382 and test accuracy is 0.6872\n",
    "\n",
    "For 128 trees and 0.5 dropout rate, train accuracy is 0.7328 and test accuracy is 0.688\n",
    "\n",
    "For 256 trees and 0.5 dropout rate, train accuracy is 0.7366 and test accuracy is 0.6868\n",
    "\n",
    "Here, for all num_trees from 16 to 256, we get essentailly the same test accuracy but 16 trees by far runs the fastest, so I would say anywhere from 16-64 trees is a good number to avoid a crazy runtime, but still get great test results, which are all considerably higher than the 0.639 that we saw in the original one tree model. Based on this procedure, it seems that more trees would lead to a lower bias and a higher variance in our predictions. Lower bias because using so many different trees will average each other's bias out and \"center us on the bullseye\" on average. However, using so many trees likely leads to higher variance because our predictions will vary greatly across the tree, and majority rule will greatly spread out any point that is near 50-50 for votes. Overall, we see these two affects approximately cancel each other out past ~16 trees, and our train and test accuracy remain about the same. \n",
    "\n",
    "\n",
    "#### Based on your results, explain how the dropout rate influences the bias and variance of the combined classifier.\n",
    "\n",
    "Results:\n",
    "\n",
    "For 64 trees and 0.1 dropout rate, train accuracy is 0.6818 and test accuracy is 0.6456\n",
    "\n",
    "For 64 trees and 0.3 dropout rate, train accuracy is 0.7034 and test accuracy is 0.663\n",
    "\n",
    "For 64 trees and 0.5 dropout rate, train accuracy is 0.7262 and test accuracy is 0.6828\n",
    "\n",
    "For 64 trees and 0.7 dropout rate, train accuracy is 0.7338 and test accuracy is 0.6596\n",
    "\n",
    "For 64 trees and 0.9 dropout rate, train accuracy is 0.6026 and test accuracy is 0.5734\n",
    "\n",
    "So rate of 0.5 is clearly the best here in terms of test performance, and both train and test accuracy decrease sharply when dropout rate increases or decreases away from 0.5. Low dropout rate has relatively higher performance, especially for the training data so I would say that low dropout rate leads to high bias, since then our predictions simply match the trees we were using before, without adding the benefit of randomness that we see with this model. Essentially, low dropout rate just fails to solve the issue that these trees are all slightly biased. We see this high bias, because for 0.1 rate, train and test accuracy are relatively high and similar, just not as good as our optimal model. High dropout rate leads to very high variance because when we only use a few predictors in each tree, we spread out our predictions and they will not be tightly packed together because we are predicting on only a very small amount of data. \n",
    "\n",
    "#### How does the test performance of an ensemble of trees fitted with the optimal dropout rate and number of trees compare with the single decision tree model in Question 1?\n",
    "\n",
    "Results printed above!\n",
    "\n",
    "With our optimal model (48 trees, 0.5 dropout rate), we get a test accuracy of 0.682, which is considerably better than the best model we saw in part 1, which had accuracy 0.639.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (15pt): Random Forests\n",
    "\n",
    "We now move to a more sophisticated ensemble technique, namely random forest:\n",
    "1. How does a random forest approach differ from the dropout procedure described in Question 2? \n",
    " \n",
    "- Fit random forest models to the training set for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracies of the models. You may set the number of predictors for each tree in the random forest model to $\\sqrt{p}$, where $p$ is the total number of predictors. \n",
    "\n",
    "- Based on your results, do you find that a larger number of trees necessarily improves the test accuracy of a random forest model? Explain how the number of trees effects the training and test accuracy of a random forest classifier, and how this relates to the bias-variance trade-off for the classifier. \n",
    "  \n",
    "- Fixing the number of trees to a reasonable value, apply 5-fold cross-validation to choose the optimal value for the  number of predictors. How does the test performance of random forest model fitted with the optimal number of trees compare with the dropout approach in Question 2?  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1\n",
    "\n",
    "A random forest approach..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "# bag_score = []\n",
    "# bag_oob = []\n",
    "rf_tr_score = []\n",
    "rf_te_score = []\n",
    "rf_oob = []\n",
    "for i in range(1,9):\n",
    "#     bag = ensemble.BaggingClassifier(n_estimators=(2**i), oob_score=True)\n",
    "#     bag.fit(X_train, Y_train)\n",
    "#     bag_score.append(bag.score(X_test, Y_test))\n",
    "#     bag_oob.append(bag.oob_score_)\n",
    "    \n",
    "    # round number of predictors, sqrt(28), up to 6\n",
    "    rf = ensemble.RandomForestClassifier(n_estimators=(2**i), \n",
    "                                         max_features = int(np.ceil(np.sqrt(X_train.shape[1]))), \n",
    "                                         oob_score=True)\n",
    "    rf.fit(X_train, Y_train)\n",
    "    rf_tr_score.append(rf.score(X_train, Y_train))\n",
    "    rf_te_score.append(rf.score(X_test, Y_test))\n",
    "    rf_oob.append(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of Trees</th>\n",
       "      <th>Train Accuracies</th>\n",
       "      <th>Test Accuracies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.8458</td>\n",
       "      <td>0.5716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9284</td>\n",
       "      <td>0.6078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.6304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.6656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>128</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>256</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # of Trees  Train Accuracies  Test Accuracies\n",
       "0           2            0.8458           0.5716\n",
       "1           4            0.9284           0.6078\n",
       "2           8            0.9804           0.6304\n",
       "3          16            0.9948           0.6656\n",
       "4          32            1.0000           0.6768\n",
       "5          64            1.0000           0.6850\n",
       "6         128            1.0000           0.6962\n",
       "7         256            1.0000           0.7000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format into data series\n",
    "number_trees_se = pd.Series([2,4,8,16,32,64,128,256])\n",
    "train_scores_se = pd.Series(rf_tr_score)\n",
    "test_scores_se = pd.Series(rf_te_score)\n",
    "# oob_depths_se = pd.Series(rf_oob)\n",
    "\n",
    "\n",
    "# create a dataframe for data presentation\n",
    "col_names = ['# of Trees', 'Train Accuracies', 'Test Accuracies']\n",
    "rf_scores_df = pd.DataFrame(columns = col_names)\n",
    "\n",
    "rf_scores_df['# of Trees'] = number_trees_se.values\n",
    "rf_scores_df['Train Accuracies'] = train_scores_se.values\n",
    "rf_scores_df['Test Accuracies'] = test_scores_se.values\n",
    "\n",
    "rf_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3\n",
    "\n",
    "Test accuracy does improve. Less bias (more accurate) but higher variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "optimal_num = 0\n",
    "\n",
    "for features in range(1,X_train.shape[1]+1):\n",
    "    rfc = ensemble.RandomForestClassifier(n_estimators=32, max_features = features, oob_score=True)\n",
    "    # Perform 5-fold cross validation \n",
    "    cv_scores = cross_val_score(rfc, X_train, Y_train, cv=5)\n",
    "    if features > 1 and np.mean(cv_scores) >= max(scores):\n",
    "        optimal_num = features\n",
    "    scores.append(np.mean(cv_scores))\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62960350180350189, 0.64900632140632131, 0.66541153021153021, 0.65740232380232377, 0.66440393180393187, 0.66780413280413276, 0.67380213960213964, 0.66400732760732761, 0.67461033901033896, 0.66780013280013273, 0.67700733920733913, 0.67120673260673258, 0.670410132010132, 0.67660414240414235, 0.67900854280854284, 0.66839753179753181, 0.67300673900673913, 0.66840753180753176, 0.66960493120493114, 0.66800953260953266, 0.6700047338047338, 0.66760353140353135, 0.67120293360293359, 0.65900312540312544, 0.67219813239813242, 0.67820074340074332, 0.66720773080773077, 0.66120752780752778]\n",
      "\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print()\n",
    "print(optimal_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9992\n",
      "0.6898\n"
     ]
    }
   ],
   "source": [
    "best_rfc = ensemble.RandomForestClassifier(n_estimators=32, max_features = optimal_num, oob_score=True)\n",
    "best_rfc.fit(X_train, Y_train)\n",
    "print(best_rfc.score(X_train, Y_train))\n",
    "print(best_rfc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (15pt): Boosting\n",
    "\n",
    "We next compare the random forest model with the approach of boosting:\n",
    "\n",
    "\n",
    "1. Apply the AdaBoost algorithm to fit an ensemble of decision trees. Set the learning rate to 0.05, and try out different tree depths for the base learners: 1, 2, 10, and unrestricted depth.  Make a plot of the training accuracy of the ensemble classifier as a function of tree depths. Make a similar plot of the test accuracies as a function of number of trees (say $2, 4, 8, 16, \\ldots, 256$).\n",
    "- How does the number of trees influence the training and test performance? Compare and contrast between the trends you see in the training and test performance of AdaBoost and that of the random forest models in Question 3. Give an explanation for your observations.\n",
    "- How does the tree depth of the base learner impact the training and test performance? Recall that with random forests, we allow the depth of the individual trees to be unrestricted. Would you recommend the same strategy for boosting? Explain your answer.\n",
    "- Apply 5-fold cross-validation to choose the optimal number of trees $B$ for the ensemble and the optimal tree depth for the base learners. How does an ensemble classifier fitted with the optimal number of trees and the optimal tree depth compare with the random forest model fitted in Question 3.4? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test num of trees and tree depth together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trees = [2,4,8,16,32,64,128,256]\n",
    "depths = [1,2,10,None]\n",
    "\n",
    "# train scores\n",
    "ab_depth_1_tr_scores = []\n",
    "ab_depth_2_tr_scores = []\n",
    "ab_depth_10_tr_scores = []\n",
    "ab_depth_no_tr_scores = []\n",
    "\n",
    "# test scores\n",
    "ab_depth_1_te_scores = []\n",
    "ab_depth_2_te_scores = []\n",
    "ab_depth_10_te_scores = []\n",
    "ab_depth_no_te_scores = []\n",
    "\n",
    "for num_trees in trees:\n",
    "    adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=1), \n",
    "                                           n_estimators=num_trees, learning_rate=0.05)\n",
    "    adaboost.fit(X_train, Y_train)\n",
    "    ab_depth_1_tr_scores.append(adaboost.score(X_train, Y_train))\n",
    "    ab_depth_1_te_scores.append(adaboost.score(X_test, Y_test))\n",
    "    \n",
    "for num_trees in trees:\n",
    "    adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=2), \n",
    "                                           n_estimators=num_trees, learning_rate=0.05)\n",
    "    adaboost.fit(X_train, Y_train)\n",
    "    ab_depth_2_tr_scores.append(adaboost.score(X_train, Y_train))\n",
    "    ab_depth_2_te_scores.append(adaboost.score(X_test, Y_test))\n",
    "    \n",
    "for num_trees in trees:\n",
    "    adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=10), \n",
    "                                           n_estimators=num_trees, learning_rate=0.05)\n",
    "    adaboost.fit(X_train, Y_train)\n",
    "    ab_depth_10_tr_scores.append(adaboost.score(X_train, Y_train))\n",
    "    ab_depth_10_te_scores.append(adaboost.score(X_test, Y_test))\n",
    "    \n",
    "for num_trees in trees:\n",
    "    adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=None), \n",
    "                                           n_estimators=num_trees, learning_rate=0.05)\n",
    "    adaboost.fit(X_train, Y_train)\n",
    "    ab_depth_no_tr_scores.append(adaboost.score(X_train, Y_train))\n",
    "    ab_depth_no_te_scores.append(adaboost.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6wAAAGDCAYAAAAvXp2OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6f8BJxd7nIDnn8YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1174dff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "ax[0].plot(ab_depth_1_tr_scores, color='red', alpha=0.8, label='Depth 1')\n",
    "ax[0].plot(ab_depth_2_tr_scores, color='blue', alpha=0.8, label='Depth 2')\n",
    "ax[0].plot(ab_depth_10_tr_scores, color='green', alpha=0.8, label='Depth 10')\n",
    "ax[0].plot(ab_depth_no_tr_scores, color='black', alpha=0.8, label='Depth None')\n",
    "ax[0].set_title('Train Accuracies')\n",
    "ax[0].set_xlabel('Log Number of Trees in Ensemble')\n",
    "ax[1].plot(ab_depth_1_te_scores, color='red', alpha=0.8, label='Depth 1')\n",
    "ax[1].plot(ab_depth_2_te_scores, color='blue', alpha=0.8, label='Depth 2')\n",
    "ax[1].plot(ab_depth_10_te_scores, color='green', alpha=0.8, label='Depth 10')\n",
    "ax[1].plot(ab_depth_no_te_scores, color='black', alpha=0.8, label='Depth None')\n",
    "ax[1].set_title('Test Accuracies')\n",
    "ax[1].set_xlabel('Log Number of Trees in Ensemble')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test num of trees and tree depth separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depths = [1,2,4,6,8,10,None]\n",
    "\n",
    "ab_depths_tr_scores = []\n",
    "ab_depths_te_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=depth), \n",
    "                                           n_estimators=32, learning_rate=0.05)\n",
    "    adaboost.fit(X_train, Y_train)\n",
    "    ab_depths_tr_scores.append(adaboost.score(X_train, Y_train))\n",
    "    ab_depths_te_scores.append(adaboost.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61680000000000001, 0.66600000000000004, 0.74480000000000002, 0.89780000000000004, 0.98599999999999999, 1.0, 1.0]\n",
      "\n",
      "[0.61960000000000004, 0.66500000000000004, 0.68679999999999997, 0.66679999999999995, 0.63060000000000005, 0.61180000000000001, 0.59840000000000004]\n"
     ]
    }
   ],
   "source": [
    "print(ab_depths_tr_scores)\n",
    "print()\n",
    "print(ab_depths_te_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trees = [2,4,8,16,32,64,128,256]\n",
    "\n",
    "ab_trees_tr_scores = []\n",
    "ab_trees_te_scores = []\n",
    "\n",
    "for num_trees in trees:\n",
    "    adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=2), \n",
    "                                           n_estimators=num_trees, learning_rate=0.05)\n",
    "    adaboost.fit(X_train, Y_train)\n",
    "    ab_trees_tr_scores.append(adaboost.score(X_train, Y_train))\n",
    "    ab_trees_te_scores.append(adaboost.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63859999999999995, 0.63859999999999995, 0.6452, 0.66000000000000003, 0.66600000000000004, 0.68520000000000003, 0.70899999999999996, 0.74019999999999997]\n",
      "\n",
      "[0.64300000000000002, 0.64300000000000002, 0.64680000000000004, 0.65680000000000005, 0.66500000000000004, 0.68359999999999999, 0.68740000000000001, 0.69399999999999995]\n"
     ]
    }
   ],
   "source": [
    "print(ab_trees_tr_scores)\n",
    "print()\n",
    "print(ab_trees_te_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.2: \n",
    "#### How does the number of trees influence the training and test performance? Compare and contrast between the trends you see in the training and test performance of AdaBoost and that of the random forest models in Question 3. Give an explanation for your observations.\n",
    "\n",
    "As num of trees goes up, both train and test perf go up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.3\n",
    "\n",
    "#### How does the tree depth of the base learner impact the training and test performance? Recall that with random forests, we allow the depth of the individual trees to be unrestricted. Would you recommend the same strategy for boosting? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.4\n",
    "\n",
    "#### Apply 5-fold cross-validation to choose the optimal number of trees $B$ for the ensemble and the optimal tree depth for the base learners. How does an ensemble classifier fitted with the optimal number of trees and the optimal tree depth compare with the random forest model fitted in Question 3.4? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-08d89f7a01a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=depth), \n\u001b[1;32m      9\u001b[0m                                                n_estimators=num_trees, learning_rate=0.05)\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mcv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madaboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv_iter)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 random_state)\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# Early termination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \"\"\"\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SAMME.R'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshkuppersmith/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimal_num_trees = 0\n",
    "optimal_tree_depth = 0\n",
    "\n",
    "scores = [0]\n",
    "\n",
    "for depth in depths:\n",
    "    for num_trees in trees:\n",
    "        adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=depth), \n",
    "                                               n_estimators=num_trees, learning_rate=0.05)\n",
    "        cv_scores = cross_val_score(adaboost, X_train, Y_train, cv=5)\n",
    "      \n",
    "        if np.mean(cv_scores) >= max(scores):\n",
    "            optimal_num_trees = num_trees\n",
    "            optimal_tree_depth = depth\n",
    "            \n",
    "        scores.append(np.mean(cv_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(optimal_num_trees)\n",
    "print(optimal_tree_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adaboost_opt = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=optimal_tree_depth), \n",
    "                                           n_estimators=optimal_num_trees, learning_rate=0.05)\n",
    "adaboost_opt.fit(X_train, Y_train)\n",
    "print(adaboost_opt.score(X_train, Y_train))\n",
    "print(adaboost_opt.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (3pt): Meta-classifier\n",
    "\n",
    "We have so far explored techniques that grow a collection of trees either by creating multiple copies of the original training set, or through a sequential procedure, and then combines these trees into a single classifier. Consider an alternate scenario where you are provided with a pre-trained collection of trees, say from different participants of a data science competition for Higgs boson discovery. What would be a good strategy to combine these pre-fitted trees into a single powerful classifier? Of course, a simple approach would be to take the majority vote from the individual trees. Can we do better than this simple combination strategy?\n",
    "\n",
    "A collection of 100 decision tree classifiers is provided in the file `models.npy` and can be loaded into an array by executing:\n",
    "\n",
    "`models = np.load('models.npy')`\n",
    "\n",
    "You can make predictions using the $i^\\text{th}$ model on an array of predictors `x` by executing:\n",
    "\n",
    "`model[i].predict(x)`  &nbsp;&nbsp;&nbsp;\n",
    "or &nbsp;&nbsp;&nbsp;\n",
    "`model[i].predict_proba(x)`\n",
    "\n",
    "and score the model on predictors `x` and labels `y` by using:\n",
    "\n",
    "`model[i].score(x, y)`.\n",
    "\n",
    "1. Implement a strategy to combine the provided decision tree classifiers, and compare the test perfomance of your approach with the majority vote classifier. Explain your strategy/algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = np.load('models.npy', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.6646\n"
     ]
    }
   ],
   "source": [
    "# baseline majority rules\n",
    "\n",
    "# put predictions together in a matrix\n",
    "test_predictions = np.zeros((len(models), len(X_test)))\n",
    "for i in range(len(models)):\n",
    "    pred = models[i].predict(X_test)\n",
    "    test_predictions[i,:] = pred\n",
    "\n",
    "# loop through, sum up predictions\n",
    "maj_prediction = []\n",
    "for i in range(len(X_test)):\n",
    "    num_1 = sum(test_predictions[:,i])\n",
    "    prob = num_1 / len(models)\n",
    "    \n",
    "    # if more than half say 1, predict as 1 else 0\n",
    "    if prob > 0.5:\n",
    "        maj_prediction.append(1.0)\n",
    "    else:\n",
    "        maj_prediction.append(0.0)\n",
    "        \n",
    "print(\"Baseline test accuracy: {}\".format(accuracy_score(maj_prediction, Y_test)))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of 1s in the training set: 0.5246\n",
      "So we will bias slighly toward selecting 1s in the model\n",
      "Biased test accuracy: 0.6646\n"
     ]
    }
   ],
   "source": [
    "# try bias toward 1, since there are more 1s in the training set\n",
    "\n",
    "# sample_percent_1 is the bias. If >0.5, bias toward predicting 1, else 0\n",
    "\n",
    "sample_percent_1 = sum(Y_train)/len(Y_train)\n",
    "print(\"Percent of 1s in the training set: {}\".format(sample_percent_1))\n",
    "print(\"So we will bias slighly toward selecting 1s in the model\")\n",
    "\n",
    "test_predictions = np.zeros((len(models), len(X_test)))\n",
    "for i in range(len(models)):\n",
    "    pred = models[i].predict(X_test)\n",
    "    test_predictions[i,:] = pred\n",
    "\n",
    "maj_prediction = []\n",
    "for i in range(len(X_test)):\n",
    "    num_1 = sum(test_predictions[:,i])\n",
    "    prob = num_1 / len(models)\n",
    "    \n",
    "    # so now, predict as 1 if more than ~0.475 say it is 1,\n",
    "    # have to be .525 confident to predict 0\n",
    "    if prob > (1 - sample_percent_1):\n",
    "        maj_prediction.append(1.0)\n",
    "    else:\n",
    "        maj_prediction.append(0.0)\n",
    "        \n",
    "print(\"Biased test accuracy: {}\".format(accuracy_score(maj_prediction, Y_test)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train acc: 0.6646 at 44 trees\n",
      "New score: 0.663\n"
     ]
    }
   ],
   "source": [
    "# majority rules, but only use some of the trees (best performing on training set)\n",
    "\n",
    "\n",
    "# collect list of scores for each model\n",
    "scores = []\n",
    "for j in range(len(models)):\n",
    "    pred = models[j].predict(X_train)\n",
    "    score = accuracy_score(pred, Y_train)\n",
    "    scores.append(score)\n",
    "    \n",
    "# select the top i scoring trees, run on training set\n",
    "overall = []\n",
    "for i in range(1,100):\n",
    "    indices = np.argpartition(scores, -i)[-i:]\n",
    "\n",
    "    train_predictions = np.zeros((len(indices), len(X_train)))\n",
    "    for i in range(len(indices)):\n",
    "        pred = models[i].predict(X_train)\n",
    "        train_predictions[i,:] = pred\n",
    "    \n",
    "    maj_prediction = []\n",
    "    for i in range(len(X_train)):\n",
    "        num_1 = sum(train_predictions[:,i])\n",
    "        prob = num_1 / len(indices)\n",
    "        if prob > 0.5:\n",
    "            maj_prediction.append(1.0)\n",
    "        else:\n",
    "            maj_prediction.append(0.0)\n",
    "            \n",
    "    overall.append(accuracy_score(maj_prediction, Y_train))\n",
    "\n",
    "# pick the # trees with the highest accuracy\n",
    "max_acc = max(overall)\n",
    "max_trees = overall.index(max(overall))\n",
    "print(\"Max train acc: {} at {} trees\".format(max_acc, max_trees))\n",
    "\n",
    "# so use 44 trees \n",
    "\n",
    "# now run on the test data with this new number of trees\n",
    "indices = np.argpartition(scores, -max_trees)[-max_trees:]\n",
    "test_predictions = np.zeros((len(indices), len(X_test)))\n",
    "for i in range(len(indices)):\n",
    "    pred = models[i].predict(X_test)\n",
    "    test_predictions[i,:] = pred\n",
    "    \n",
    "    maj_prediction = []\n",
    "    for i in range(len(X_test)):\n",
    "        num_1 = sum(test_predictions[:,i])\n",
    "        prob = num_1 / len(indices)\n",
    "        if prob > 0.5:\n",
    "            maj_prediction.append(1.0)\n",
    "        else:\n",
    "            maj_prediction.append(0.0)\n",
    "            \n",
    "print(\"New score: {}\".format(accuracy_score(maj_prediction, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of 1s in the training set: 0.5246\n",
      "So we will bias slighly toward selecting 1s in the model\n",
      "Max train acc: 0.6638 at 44 trees\n",
      "New score: 0.6726\n",
      "So combining the two methods (using only the 44 'best' trees and biasing toward 1 due to the number of 1s) was successful in improving the accuracy of this model!\n"
     ]
    }
   ],
   "source": [
    "# we will only use some of the trees (best performing on training set)\n",
    "# in addition, we will bias this time toward 1 instead of majority rules\n",
    "# essentially comining the last two methods\n",
    "\n",
    "sample_percent_1 = sum(Y_train)/len(Y_train)\n",
    "print(\"Percent of 1s in the training set: {}\".format(sample_percent_1))\n",
    "print(\"So we will bias slighly toward selecting 1s in the model\")\n",
    "\n",
    "# collect scores\n",
    "scores = []\n",
    "for j in range(len(models)):\n",
    "    pred = models[j].predict(X_train)\n",
    "    score = accuracy_score(pred, Y_train)\n",
    "    scores.append(score)\n",
    "    \n",
    "# select the top i scoring trees, run on training set\n",
    "overall = []\n",
    "for i in range(1,100):\n",
    "    indices = np.argpartition(scores, -i)[-i:]\n",
    "\n",
    "    train_predictions = np.zeros((len(indices), len(X_train)))\n",
    "    for i in range(len(indices)):\n",
    "        pred = models[i].predict(X_train)\n",
    "        train_predictions[i,:] = pred\n",
    "    \n",
    "    maj_prediction = []\n",
    "    for i in range(len(X_train)):\n",
    "        num_1 = sum(train_predictions[:,i])\n",
    "        prob = num_1 / len(indices)\n",
    "        \n",
    "        # include bias toward 1\n",
    "        if prob > (1 - sample_percent_1):\n",
    "            maj_prediction.append(1.0)\n",
    "        else:\n",
    "            maj_prediction.append(0.0)\n",
    "            \n",
    "    overall.append(accuracy_score(maj_prediction, Y_train))\n",
    "    \n",
    "# pick the # trees with the highest accuracy on training se\n",
    "max_acc = max(overall)\n",
    "max_trees = overall.index(max(overall))\n",
    "print(\"Max train acc: {} at {} trees\".format(max_acc, max_trees))\n",
    "\n",
    "# so use 44 trees \n",
    "\n",
    "# now run on test data\n",
    "indices = np.argpartition(scores, -max_trees)[-max_trees:]\n",
    "test_predictions = np.zeros((len(indices), len(X_test)))\n",
    "for i in range(len(indices)):\n",
    "    pred = models[i].predict(X_test)\n",
    "    test_predictions[i,:] = pred\n",
    "    \n",
    "    maj_prediction = []\n",
    "    for i in range(len(X_test)):\n",
    "        num_1 = sum(test_predictions[:,i])\n",
    "        prob = num_1 / len(indices)\n",
    "        \n",
    "        # bias toward 1\n",
    "        if prob > (1 - sample_percent_1):\n",
    "            maj_prediction.append(1.0)\n",
    "        else:\n",
    "            maj_prediction.append(0.0)\n",
    "            \n",
    "print(\"New score: {}\".format(accuracy_score(maj_prediction, Y_test)))\n",
    "\n",
    "print(\"So combining the two methods (using only the 44 'best' trees and biasing toward 1 due to the number of 1s) was successful in improving the accuracy of this model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "\n",
    "#### Explain your strategy/algorithm.\n",
    "\n",
    "Here, I experimented and found two measures that essentially kept the test accuracy the same. The first was to find an optimal i for which i selected the top i individually performing trees, and ran majority rules with this many trees. The second was to introduce bias, so that I had a higher threshold majority needed to select a 0 (since 0s are overall less likely in the training set). Neither of these methods improved my baseline accuracy. However, using both together actually provides an edge over the baseline accuracy (0.6646) and boosts it to 0.6726! So these methods together allow for smarter use of these pre-trained classification trees.\n",
    "\n",
    "Note: some other things that I tried were: bagging, VotingClassifier and GridSearchCV from sklearn, and introducing randomness when the majority vote was unclear (this actually improved performance on some trials, but was obviously inconsistent due to its random nature). In the end, the above method was the most successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "---\n",
    "\n",
    "## APCOMP209a - Homework Question\n",
    "​\n",
    "We've worked with imputation methods on missing data in Homework 6.  We've worked with Decision Trees in HW7 and here.  Now let's see what happens if we try to work with Decision Trees and Missing Data at the same time! We'll be working with a dataset from the UCI Machine Learning Repository that uses a variety of wine chemical predictors to classify wines grown in the same region in Italy.  Each line represents 13 (mostly chemical) predictors of the response variable wine class, including things like alcohol content, hue , and phenols.  Unfortunately some of the predictor values were lost in measurement. Please load `wine_quality_missing.csv`. \n",
    "​\n",
    "*Note*: As in HW6 be careful of reading/treating column names and row names in this data set.\n",
    "​\n",
    "​\n",
    "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the sklearn DecisionTreeClassifier and RandomForestClassifier.   Use cross-validation to find the optimal tree depth for each method.  Report the optimal tree-depth, overall classification rate and confusion matrix on the test set for each method.\n",
    "2. Restart with a fresh copy of the data and impute the missing data via mean imputation.  Split the data 75-25 and again fit DecisionTreeClassifier and RandomForestClassifier using cross-validation to find the optimal tree depth.  Report the optimal tree depth, overall classification rate and confusion matrix on the test set for each method.  \n",
    "3. Again restart with a fresh copy of the data but this time let's try something different.  As discussed in section, CART Decision Trees can take advantage of surrogate splits to handle missing data.  Split the data 75-25 and construct a **custom** decision tree model and train it on the training set with missing data. Report the optimal tree depth, overall classification rate and confusion matrix on the test set and compare your results to the Imputation and DecisionTree model results in part 1 & 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
